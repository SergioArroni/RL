{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TT3LPGc5PuK"
      },
      "source": [
        "\n",
        "\n",
        "<p><img height=\"80px\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/UPM/Escudo/EscUpm.jpg\" align=\"left\" hspace=\"0px\" vspace=\"0px\"></p>\n",
        "\n",
        "**Course \"Artificial Neural Networks and Deep Learning\" - Universidad Politécnica de Madrid (UPM)**\n",
        "\n",
        "# **Deep Q-Learning for Cartpole**\n",
        "\n",
        "This notebook includes an implementation of the Deep Q-learning (DQN) algorithm for the cartpole problem (see [Cartpole documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/)).\n",
        "\n",
        "Original code by: Artificial Neural Networks and Deep Learning professors\n",
        "Modifications by:\n",
        "<ul>\n",
        "<li>David González Fernández (david.gonzalezf@alumnos.upm.es)</li>\n",
        "<li>Sergio Arroni del Riego ()</li>\n",
        "<li>José Manuel Pérez Lamas ()</li>\n",
        "<li>Paul Delage (paul.delage@alumnos.upm.es)</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Listado de cambios realizados:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXBzOdaLAEUn"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjLS1WetFhCE",
        "outputId": "0a4dc719-9cc7-4cd2-be7b-04716a4dd123"
      },
      "outputs": [],
      "source": [
        "# TODO: descomentar\n",
        "#!pip install gymnasium[classic-control]\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from collections import namedtuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.debugging.set_log_device_placement(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBrRuhN1AQ-s"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4N2yVwtuFlBu"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.99\n",
        "MEMORY_SIZE = 50000\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "EXPLORATION_MAX = 1\n",
        "EXPLORATION_MIN = 0.05\n",
        "EXPLORATION_DECAY = 0.99\n",
        "NUMBER_OF_EPISODES_FOR_TRAINING = 3000\n",
        "NUMBER_OF_EPISODES_FOR_TESTING = 20\n",
        "MAX_STEPS_PER_EPISODE = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoGaas6TAd6p"
      },
      "source": [
        "## Class ReplayMemory\n",
        "\n",
        "Memory of transitions for experience replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'state_next', 'reward', 'terminal_state'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cQV7IfhFOoSh"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self,number_of_observations):\n",
        "        # Create replay memory\n",
        "        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n",
        "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n",
        "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
        "        self.rewards = np.zeros(MEMORY_SIZE)\n",
        "        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n",
        "        self.current_size = 0\n",
        "        self.position = 0\n",
        "        self.max_size = MEMORY_SIZE\n",
        "\n",
        "\n",
        "        self.memory = []\n",
        "        self.rng = np.random.default_rng()\n",
        "\n",
        "    # def store_transition(self, state, action, reward, state_next, terminal_state):\n",
        "    #     # Store a transition (s,a,r,s') in the replay memory\n",
        "    #     i = (self.position) % self.max_size\n",
        "    #     self.position += 1\n",
        "    #     self.states[i] = state\n",
        "    #     self.states_next[i] = state_next\n",
        "    #     self.actions[i] = action\n",
        "    #     self.rewards[i] = reward\n",
        "    #     self.terminal_states[i] = terminal_state\n",
        "    #     self.current_size = min(self.current_size+1, self.max_size)\n",
        "\n",
        "    # def push(self, *args):\n",
        "    #     \"\"\"Saves a transition.\"\"\"\n",
        "    #     if len(self.memory) < self.capacity:\n",
        "    #         self.memory.append(None)\n",
        "    #     self.memory[self.position] = Transition(*args)\n",
        "    #     self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "\n",
        "    # self.memory.push(\n",
        "    #         torch.tensor([f_state], device=self.device),\n",
        "    #         torch.tensor([action], device=self.device, dtype=torch.long),\n",
        "    #         torch.tensor([next_state], device=self.device),\n",
        "    #         torch.tensor([reward], device=self.device),\n",
        "    #         torch.tensor([done], device=self.device, dtype=torch.bool),\n",
        "    # )\n",
        "\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
        "        #with tf.device('/gpu:0'):\n",
        "            # Store a transition (s,a,r,s') in the replay memory\n",
        "        i = (self.position) % self.max_size\n",
        "        self.position += 1\n",
        "        self.memory.append(Transition(state, action, state_next, reward, terminal_state))\n",
        "            # self.states[i] = state\n",
        "            # self.states_next[i] = state_next\n",
        "            # self.actions[i] = action\n",
        "            # self.rewards[i] = reward\n",
        "            # self.terminal_states[i] = terminal_state\n",
        "        self.current_size = min(self.current_size+1, self.max_size)\n",
        "\n",
        "    # def sample_memory(self, batch_size):\n",
        "    #     # Generate a sample of transitions from the replay memory\n",
        "    #     batch = np.random.choice(self.current_size, batch_size)\n",
        "    #     states = self.states[batch]\n",
        "    #     states_next = self.states_next[batch]\n",
        "    #     rewards = self.rewards[batch]\n",
        "    #     actions = self.actions[batch]\n",
        "    #     terminal_states = self.terminal_states[batch]\n",
        "    #     return states, actions, rewards, states_next, terminal_states\n",
        "    \n",
        "    # transitions = self.memory.sample(self.batch_size)\n",
        "    # batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # state_batch = torch.cat(batch.state)\n",
        "    # action_batch = torch.cat(batch.action)\n",
        "    # reward_batch = torch.cat(batch.reward)\n",
        "    # next_state_batch = torch.cat(batch.next_state)\n",
        "    # done_batch = torch.cat(batch.done)\n",
        "    \n",
        "    def sample_memory(self, batch_size):\n",
        "        idx = self.rng.choice(np.arange(len(self.memory)),\n",
        "                              batch_size, replace=False)\n",
        "        res = []\n",
        "        for i in idx:\n",
        "            res.append(self.memory[i])\n",
        "        batch = Transition(*zip(*res))\n",
        "        states = np.concatenate(batch.state)\n",
        "        states_next = np.concatenate(batch.state_next)\n",
        "        rewards = np.concatenate(batch.reward)\n",
        "        actions = np.concatenate(batch.action)\n",
        "        terminal_states = np.concatenate(batch.terminal_state)\n",
        "        # states = tf.convert_to_tensor(batch.state)\n",
        "        # states_next = tf.convert_to_tensor(batch.state_next)\n",
        "        # rewards = tf.convert_to_tensor(batch.reward)\n",
        "        # actions = tf.convert_to_tensor(batch.action)\n",
        "        # terminal_states = tf.convert_to_tensor(batch.terminal_state)\n",
        "        return states, actions, rewards, states_next, terminal_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gejKO0OYAsS4"
      },
      "source": [
        "## Class DQN\n",
        "\n",
        "Reinforcement learning agent with a Deep Q-Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NZ6P4Gj0FtnU"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, number_of_observations, number_of_actions):\n",
        "        # Initialize variables and create neural model\n",
        "        self.exploration_rate = EXPLORATION_MAX\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.number_of_observations = number_of_observations\n",
        "        self.scores = []\n",
        "\n",
        "        self.memory = ReplayMemory(number_of_observations)\n",
        "        self.warm_up_episodes = 25\n",
        "\n",
        "        # Modelo\n",
        "        self.model = keras.models.Sequential()\n",
        "        self.model.add(keras.layers.Embedding(number_of_observations, 6))\n",
        "        self.model.add(keras.layers.Dense(50, activation='relu'))\n",
        "        self.model.add(keras.layers.Dense(50, activation='relu'))\n",
        "        self.model.add(keras.layers.Dense(50, activation='relu'))\n",
        "        self.model.add(keras.layers.Dense(number_of_actions, activation='relu'))\n",
        "        self.model.compile(loss=\"mse\", optimizer=keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE))\n",
        "\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, terminal_state):\n",
        "        # Store a tuple (s, a, r, s') for experience replay\n",
        "        state = np.reshape(state, [1, 1])\n",
        "        next_state = np.reshape(next_state, [1, 1])\n",
        "        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n",
        "\n",
        "\n",
        "    def select(self, state):\n",
        "        # Generate an action for a given state using epsilon-greedy policy\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return random.randrange(self.number_of_actions)\n",
        "        else:\n",
        "            state = np.reshape(state, [1, 1])\n",
        "            q_values = self.model.predict(state, verbose=0)\n",
        "            return np.argmax(q_values[0])\n",
        "\n",
        "\n",
        "    def select_greedy_policy(self, state):\n",
        "        # Generate an action for a given state using greedy policy\n",
        "        state = np.reshape(state, [1, 1])\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "\n",
        "    # TODO: cambiar\n",
        "    def learn(self):\n",
        "        # Learn the value Q using a sample of examples from the replay memory\n",
        "        if self.memory.current_size < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n",
        "\n",
        "        # print('Loop 1 :')\n",
        "        # print('states_batch_len', len(states))\n",
        "        # print('actions_batch_len', len(actions))\n",
        "        # print('rewards_batch_len', len(rewards))\n",
        "        # print('next_states_batch_len', len(next_states))\n",
        "\n",
        "        q_targets = self.model.predict(states, verbose=0)\n",
        "        q_next_states = self.model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(BATCH_SIZE):\n",
        "             if (terminal_states[i]):\n",
        "                q_targets[i][actions[i]] = rewards[i]\n",
        "             else:\n",
        "                q_targets[i][actions[i]] = rewards[i] + GAMMA * np.max(q_next_states[i])\n",
        "\n",
        "        self.model.train_on_batch(states, q_targets)\n",
        "    \n",
        "\n",
        "    def decrease_exploration_rate(self, n_episode):\n",
        "        if n_episode >= self.warm_up_episodes:\n",
        "            # Decrease exploration rate\n",
        "            self.exploration_rate *= EXPLORATION_DECAY\n",
        "            self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
        "\n",
        "\n",
        "    def add_score(self, score):\n",
        "       # Add the obtained score to a list to be presented later\n",
        "        self.scores.append(score)\n",
        "\n",
        "\n",
        "    def delete_scores(self):\n",
        "       # Delete the scores\n",
        "        self.scores = []\n",
        "\n",
        "\n",
        "    def display_scores_graphically(self):\n",
        "        # Display the obtained scores graphically\n",
        "        plt.plot(self.scores)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-YSpziT0K9I"
      },
      "source": [
        "## Environment Cartpole\n",
        "\n",
        "<p><img height=\"200px\" src=\"https://raw.githubusercontent.com/martin-molina/reinforcement_learning/main/images/cartpole_attributes.png\" align=\"center\" vspace=\"20px\"</p>\n",
        "\n",
        "State vector:\n",
        "- state[0]: cart position\n",
        "- state[1]: cart velocity\n",
        "- state[2]: pole angle\n",
        "- state[3]: pole angular velocity\n",
        "\n",
        "Actions:\n",
        "- 0 (push cart to the left)\n",
        "- 1 (push cart to the right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4LBloUSG0LmT"
      },
      "outputs": [],
      "source": [
        "def create_environment():\n",
        "    # Create simulated environment\n",
        "    environment = gym.make(\"Taxi-v3\")\n",
        "    number_of_observations = environment.observation_space.n\n",
        "    number_of_actions = environment.action_space.n\n",
        "    return environment, number_of_observations, number_of_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbbw6blhDcsJ"
      },
      "source": [
        "## Training program\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yuzI0m5u5vVf",
        "outputId": "a7872c2c-0cfd-4979-ee43-14ce73a48b7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-17 15:21:45.085963: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
            "2024-01-17 15:21:45.085983: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2024-01-17 15:21:45.085987: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2024-01-17 15:21:45.086015: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-01-17 15:21:45.086033: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode   1: score -371 (exploration rate: 1.00, transitions: 101)\n",
            "128\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "zero-dimensional arrays cannot be concatenated",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore (average last 10 episodes):\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_score)\n\u001b[1;32m     52\u001b[0m     agent\u001b[38;5;241m.\u001b[39mdisplay_scores_graphically()\n\u001b[0;32m---> 55\u001b[0m borrar()\n",
            "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36mborrar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Learn using a batch of experience stored in memory\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# agent.learn()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Detect end of episode\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminal_state \u001b[38;5;129;01mor\u001b[39;00m truncated \u001b[38;5;129;01mor\u001b[39;00m steps\u001b[38;5;241m>\u001b[39mMAX_STEPS_PER_EPISODE:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Learn using a batch of experience stored in memory\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn()\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# TODO: pendiente de comprobar\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     agent\u001b[38;5;241m.\u001b[39madd_score(score)\n",
            "Cell \u001b[0;32mIn[7], line 52\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mcurrent_size \u001b[38;5;241m<\u001b[39m BATCH_SIZE:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m states, actions, rewards, next_states, terminal_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample_memory(BATCH_SIZE)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print('Loop 1 :')\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# print('states_batch_len', len(states))\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print('actions_batch_len', len(actions))\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print('rewards_batch_len', len(rewards))\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print('next_states_batch_len', len(next_states))\u001b[39;00m\n\u001b[1;32m     60\u001b[0m q_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(states, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 88\u001b[0m, in \u001b[0;36mReplayMemory.sample_memory\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch\u001b[38;5;241m.\u001b[39mreward)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(rewards)\n\u001b[0;32m---> 88\u001b[0m actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(batch\u001b[38;5;241m.\u001b[39maction)\n\u001b[1;32m     89\u001b[0m terminal_states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(batch\u001b[38;5;241m.\u001b[39mterminal_state)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# states = tf.convert_to_tensor(batch.state)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# states_next = tf.convert_to_tensor(batch.state_next)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# rewards = tf.convert_to_tensor(batch.reward)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# actions = tf.convert_to_tensor(batch.action)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# terminal_states = tf.convert_to_tensor(batch.terminal_state)\u001b[39;00m\n",
            "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
          ]
        }
      ],
      "source": [
        "def borrar():\n",
        "    environment, number_of_observations, number_of_actions = create_environment()\n",
        "    agent = DQN(number_of_observations, number_of_actions)\n",
        "    episode = 0\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    while (episode < NUMBER_OF_EPISODES_FOR_TRAINING):\n",
        "        episode += 1\n",
        "        score = 0\n",
        "        state, info = environment.reset()\n",
        "        end_episode = False\n",
        "        steps = 0\n",
        "        agent.decrease_exploration_rate(episode)\n",
        "        \n",
        "        while not(end_episode):\n",
        "            steps += 1\n",
        "            # Select an action for the current state\n",
        "            action = agent.select(state)\n",
        "\n",
        "            # Execute the action on the environment\n",
        "            state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
        "        \n",
        "            # Store in memory the transition (s,a,r,s')\n",
        "            agent.remember(state, action, reward, state_next, terminal_state)\n",
        "\n",
        "            score += reward\n",
        "\n",
        "            # Learn using a batch of experience stored in memory\n",
        "            # agent.learn()\n",
        "\n",
        "            # Detect end of episode\n",
        "            if terminal_state or truncated or steps>MAX_STEPS_PER_EPISODE:\n",
        "                # Learn using a batch of experience stored in memory\n",
        "                agent.learn()\n",
        "\n",
        "                # TODO: pendiente de comprobar\n",
        "                agent.add_score(score)\n",
        "                print(\"Episode {0:>3}: \".format(episode), end = '')\n",
        "                print(\"score {0:>3} \".format(math.trunc(score)), end = '')\n",
        "                print(\"(exploration rate: %.2f, \" % agent.exploration_rate, end = '')\n",
        "                print(\"transitions: \" + str(agent.memory.current_size) + \")\")\n",
        "                end_episode = True\n",
        "            else:\n",
        "                state = state_next\n",
        "\n",
        "\n",
        "    print(\"Time for training:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n",
        "    print(\"Score (max):\", max(agent.scores))\n",
        "    average_score = np.mean(agent.scores[max(0,(len(agent.scores)-10)):(len(agent.scores))])\n",
        "    print(\"Score (average last 10 episodes):\", average_score)\n",
        "\n",
        "    agent.display_scores_graphically()\n",
        "\n",
        "\n",
        "borrar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSteJT6ULQVG"
      },
      "source": [
        "\n",
        "## Testing program\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIP0LQHGLZPj",
        "outputId": "906f931f-d01b-4147-f802-e3ea45f44045"
      },
      "source": [
        "agent.delete_scores()\n",
        "episode = 0\n",
        "start_time = time.perf_counter()\n",
        "while (episode < NUMBER_OF_EPISODES_FOR_TESTING):\n",
        "    episode += 1\n",
        "    score = 0\n",
        "    state, info = environment.reset()\n",
        "    end_episode = False\n",
        "    while not(end_episode):\n",
        "        # Select an action for the current state\n",
        "        action = agent.select_greedy_policy(state)\n",
        "\n",
        "        # Execute the action in the environment\n",
        "        state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        # Detect end of episode and print\n",
        "        if terminal_state or truncated:\n",
        "            agent.add_score(score)\n",
        "            print(\"Episode {0:>3}: \".format(episode), end = '')\n",
        "            print(\"score {0:>3} \\n\".format(math.trunc(score)), end = '')\n",
        "            end_episode = True\n",
        "        else:\n",
        "            state = state_next\n",
        "\n",
        "    print(\"Time for testing:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n",
        "    print(\"Score (average):\", np.mean(agent.scores))\n",
        "    print(\"Score (max):\", max(agent.scores))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
