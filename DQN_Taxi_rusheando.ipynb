{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TT3LPGc5PuK"
      },
      "source": [
        "\n",
        "\n",
        "<p><img height=\"80px\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/UPM/Escudo/EscUpm.jpg\" align=\"left\" hspace=\"0px\" vspace=\"0px\"></p>\n",
        "\n",
        "**Course \"Artificial Neural Networks and Deep Learning\" - Universidad Politécnica de Madrid (UPM)**\n",
        "\n",
        "# **Deep Q-Learning for Cartpole**\n",
        "\n",
        "This notebook includes an implementation of the Deep Q-learning (DQN) algorithm for the cartpole problem (see [Cartpole documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/)).\n",
        "\n",
        "Original code by: Artificial Neural Networks and Deep Learning professors\n",
        "Modifications by:\n",
        "<ul>\n",
        "<li>David González Fernández (david.gonzalezf@alumnos.upm.es)</li>\n",
        "<li>Sergio Arroni del Riego ()</li>\n",
        "<li>José Manuel Pérez Lamas ()</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Listado de cambios realizados:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXBzOdaLAEUn"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjLS1WetFhCE",
        "outputId": "0a4dc719-9cc7-4cd2-be7b-04716a4dd123"
      },
      "outputs": [],
      "source": [
        "# TODO: descomentar\n",
        "#!pip install gymnasium[classic-control]\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBrRuhN1AQ-s"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "4N2yVwtuFlBu"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.99\n",
        "MEMORY_SIZE = 50000\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "EXPLORATION_MAX = 1\n",
        "EXPLORATION_MIN = 0.05\n",
        "EXPLORATION_DECAY = 0.99\n",
        "NUMBER_OF_EPISODES_FOR_TRAINING = 3000\n",
        "NUMBER_OF_EPISODES_FOR_TESTING = 20\n",
        "MAX_STEPS_PER_EPISODE = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoGaas6TAd6p"
      },
      "source": [
        "## Class ReplayMemory\n",
        "\n",
        "Memory of transitions for experience replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "cQV7IfhFOoSh"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self,number_of_observations):\n",
        "        # Create replay memory\n",
        "        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n",
        "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n",
        "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
        "        self.rewards = np.zeros(MEMORY_SIZE)\n",
        "        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n",
        "        self.current_size = 0\n",
        "        self.position = 0\n",
        "        self.max_size = MEMORY_SIZE\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
        "        # Store a transition (s,a,r,s') in the replay memory\n",
        "        i = (self.position) % self.max_size\n",
        "        self.position += 1\n",
        "        self.states[i] = state\n",
        "        self.states_next[i] = state_next\n",
        "        self.actions[i] = action\n",
        "        self.rewards[i] = reward\n",
        "        self.terminal_states[i] = terminal_state\n",
        "        self.current_size = min(self.current_size+1, self.max_size)\n",
        "\n",
        "    def sample_memory(self, batch_size):\n",
        "        # Generate a sample of transitions from the replay memory\n",
        "        batch = np.random.choice(self.current_size, batch_size)\n",
        "        states = self.states[batch]\n",
        "        states_next = self.states_next[batch]\n",
        "        rewards = self.rewards[batch]\n",
        "        actions = self.actions[batch]\n",
        "        terminal_states = self.terminal_states[batch]\n",
        "        return states, actions, rewards, states_next, terminal_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gejKO0OYAsS4"
      },
      "source": [
        "## Class DQN\n",
        "\n",
        "Reinforcement learning agent with a Deep Q-Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "NZ6P4Gj0FtnU"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, number_of_observations, number_of_actions):\n",
        "        # Initialize variables and create neural model\n",
        "        self.exploration_rate = EXPLORATION_MAX\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.number_of_observations = number_of_observations\n",
        "        self.scores = []\n",
        "\n",
        "        self.memory = ReplayMemory(number_of_observations)\n",
        "        self.warm_up_episodes = 25\n",
        "\n",
        "        # Modelo\n",
        "        self.model = keras.models.Sequential()\n",
        "        self.model.add(keras.layers.Embedding(number_of_observations, 6))\n",
        "        self.model.add(keras.layers.Dense(50, activation='relu'))\n",
        "        self.model.add(keras.layers.Dense(50, activation='relu'))\n",
        "        self.model.add(keras.layers.Dense(50, activation='relu'))\n",
        "        self.model.add(keras.layers.Dense(number_of_actions, activation='relu'))\n",
        "        self.model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
        "\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, terminal_state):\n",
        "        # Store a tuple (s, a, r, s') for experience replay\n",
        "        state = np.reshape(state, [1, 1])\n",
        "        next_state = np.reshape(next_state, [1, 1])\n",
        "        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n",
        "\n",
        "\n",
        "    def select(self, state):\n",
        "        # Generate an action for a given state using epsilon-greedy policy\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return random.randrange(self.number_of_actions)\n",
        "        else:\n",
        "            state = np.reshape(state, [1, 1])\n",
        "            q_values = self.model.predict(state, verbose=0)\n",
        "            return np.argmax(q_values[0])\n",
        "\n",
        "\n",
        "    def select_greedy_policy(self, state):\n",
        "        # Generate an action for a given state using greedy policy\n",
        "        state = np.reshape(state, [1, 1])\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "\n",
        "    # TODO: cambiar\n",
        "    def learn(self):\n",
        "        # Learn the value Q using a sample of examples from the replay memory\n",
        "        if self.memory.current_size < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n",
        "\n",
        "        q_targets = self.model.predict(states, verbose=0)\n",
        "        q_next_states = self.model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(BATCH_SIZE):\n",
        "             if (terminal_states[i]):\n",
        "                  q_targets[i][actions[i]] = rewards[i]\n",
        "             else:\n",
        "                  q_targets[i][actions[i]] = rewards[i] + GAMMA * np.max(q_next_states[i])\n",
        "\n",
        "        self.model.train_on_batch(states, q_targets)\n",
        "    \n",
        "\n",
        "    def decrease_exploration_rate(self, n_episode):\n",
        "        if n_episode >= self.warm_up_episodes:\n",
        "            # Decrease exploration rate\n",
        "            self.exploration_rate *= EXPLORATION_DECAY\n",
        "            self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
        "\n",
        "\n",
        "    def add_score(self, score):\n",
        "       # Add the obtained score to a list to be presented later\n",
        "        self.scores.append(score)\n",
        "\n",
        "\n",
        "    def delete_scores(self):\n",
        "       # Delete the scores\n",
        "        self.scores = []\n",
        "\n",
        "\n",
        "    def display_scores_graphically(self):\n",
        "        # Display the obtained scores graphically\n",
        "        plt.plot(self.scores)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-YSpziT0K9I"
      },
      "source": [
        "## Environment Cartpole\n",
        "\n",
        "<p><img height=\"200px\" src=\"https://raw.githubusercontent.com/martin-molina/reinforcement_learning/main/images/cartpole_attributes.png\" align=\"center\" vspace=\"20px\"</p>\n",
        "\n",
        "State vector:\n",
        "- state[0]: cart position\n",
        "- state[1]: cart velocity\n",
        "- state[2]: pole angle\n",
        "- state[3]: pole angular velocity\n",
        "\n",
        "Actions:\n",
        "- 0 (push cart to the left)\n",
        "- 1 (push cart to the right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "4LBloUSG0LmT"
      },
      "outputs": [],
      "source": [
        "def create_environment():\n",
        "    # Create simulated environment\n",
        "    environment = gym.make(\"Taxi-v3\")\n",
        "    number_of_observations = environment.observation_space.n\n",
        "    number_of_actions = environment.action_space.n\n",
        "    return environment, number_of_observations, number_of_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbbw6blhDcsJ"
      },
      "source": [
        "## Training program\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yuzI0m5u5vVf",
        "outputId": "a7872c2c-0cfd-4979-ee43-14ce73a48b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode   1: score -407 (exploration rate: 1.00, transitions: 101)\n",
            "Episode   2: score -407 (exploration rate: 1.00, transitions: 202)\n",
            "Episode   3: score -354 (exploration rate: 1.00, transitions: 298)\n",
            "Episode   4: score -434 (exploration rate: 1.00, transitions: 399)\n",
            "Episode   5: score -425 (exploration rate: 1.00, transitions: 500)\n",
            "Episode   6: score -344 (exploration rate: 1.00, transitions: 601)\n",
            "Episode   7: score -362 (exploration rate: 1.00, transitions: 702)\n",
            "Episode   8: score -398 (exploration rate: 1.00, transitions: 803)\n",
            "Episode   9: score -434 (exploration rate: 1.00, transitions: 904)\n",
            "Episode  10: score -380 (exploration rate: 1.00, transitions: 1005)\n",
            "Episode  11: score -398 (exploration rate: 1.00, transitions: 1106)\n",
            "Episode  12: score -371 (exploration rate: 1.00, transitions: 1207)\n",
            "Episode  13: score -362 (exploration rate: 1.00, transitions: 1308)\n",
            "Episode  14: score -434 (exploration rate: 1.00, transitions: 1409)\n",
            "Episode  15: score -398 (exploration rate: 1.00, transitions: 1510)\n",
            "Episode  16: score -353 (exploration rate: 1.00, transitions: 1611)\n",
            "Episode  17: score -461 (exploration rate: 1.00, transitions: 1712)\n",
            "Episode  18: score -434 (exploration rate: 1.00, transitions: 1813)\n",
            "Episode  19: score -425 (exploration rate: 1.00, transitions: 1914)\n",
            "Episode  20: score -344 (exploration rate: 1.00, transitions: 2015)\n",
            "Episode  21: score -344 (exploration rate: 1.00, transitions: 2116)\n",
            "Episode  22: score -407 (exploration rate: 1.00, transitions: 2217)\n",
            "Episode  23: score -506 (exploration rate: 1.00, transitions: 2318)\n",
            "Episode  24: score -353 (exploration rate: 1.00, transitions: 2419)\n",
            "Episode  25: score -515 (exploration rate: 0.99, transitions: 2520)\n",
            "Episode  26: score -362 (exploration rate: 0.98, transitions: 2621)\n",
            "Episode  27: score -443 (exploration rate: 0.97, transitions: 2722)\n",
            "Episode  28: score -452 (exploration rate: 0.96, transitions: 2823)\n",
            "Episode  29: score -344 (exploration rate: 0.95, transitions: 2924)\n",
            "Episode  30: score -326 (exploration rate: 0.94, transitions: 3025)\n",
            "Episode  31: score -398 (exploration rate: 0.93, transitions: 3126)\n",
            "Episode  32: score -389 (exploration rate: 0.92, transitions: 3227)\n",
            "Episode  33: score -335 (exploration rate: 0.91, transitions: 3328)\n",
            "Episode  34: score -326 (exploration rate: 0.90, transitions: 3429)\n",
            "Episode  35: score -362 (exploration rate: 0.90, transitions: 3530)\n",
            "Episode  36: score -371 (exploration rate: 0.89, transitions: 3631)\n",
            "Episode  37: score -362 (exploration rate: 0.88, transitions: 3732)\n",
            "Episode  38: score -353 (exploration rate: 0.87, transitions: 3833)\n",
            "Episode  39: score -425 (exploration rate: 0.86, transitions: 3934)\n",
            "Episode  40: score -272 (exploration rate: 0.85, transitions: 4035)\n",
            "Episode  41: score -416 (exploration rate: 0.84, transitions: 4136)\n",
            "Episode  42: score -407 (exploration rate: 0.83, transitions: 4237)\n",
            "Episode  43: score -308 (exploration rate: 0.83, transitions: 4338)\n",
            "Episode  44: score -398 (exploration rate: 0.82, transitions: 4439)\n",
            "Episode  45: score -344 (exploration rate: 0.81, transitions: 4540)\n",
            "Episode  46: score -326 (exploration rate: 0.80, transitions: 4641)\n",
            "Episode  47: score -290 (exploration rate: 0.79, transitions: 4742)\n",
            "Episode  48: score -335 (exploration rate: 0.79, transitions: 4843)\n",
            "Episode  49: score -236 (exploration rate: 0.78, transitions: 4944)\n",
            "Episode  50: score -326 (exploration rate: 0.77, transitions: 5045)\n",
            "Episode  51: score -326 (exploration rate: 0.76, transitions: 5146)\n",
            "Episode  52: score -272 (exploration rate: 0.75, transitions: 5247)\n",
            "Episode  53: score -353 (exploration rate: 0.75, transitions: 5348)\n",
            "Episode  54: score -344 (exploration rate: 0.74, transitions: 5449)\n",
            "Episode  55: score -290 (exploration rate: 0.73, transitions: 5550)\n",
            "Episode  56: score -326 (exploration rate: 0.72, transitions: 5651)\n",
            "Episode  57: score -371 (exploration rate: 0.72, transitions: 5752)\n",
            "Episode  58: score -272 (exploration rate: 0.71, transitions: 5853)\n",
            "Episode  59: score -353 (exploration rate: 0.70, transitions: 5954)\n",
            "Episode  60: score -326 (exploration rate: 0.70, transitions: 6055)\n",
            "Episode  61: score -308 (exploration rate: 0.69, transitions: 6156)\n",
            "Episode  62: score -308 (exploration rate: 0.68, transitions: 6257)\n",
            "Episode  63: score -272 (exploration rate: 0.68, transitions: 6358)\n",
            "Episode  64: score -218 (exploration rate: 0.67, transitions: 6459)\n",
            "Episode  65: score -299 (exploration rate: 0.66, transitions: 6560)\n",
            "Episode  66: score -245 (exploration rate: 0.66, transitions: 6661)\n",
            "Episode  67: score -290 (exploration rate: 0.65, transitions: 6762)\n",
            "Episode  68: score -308 (exploration rate: 0.64, transitions: 6863)\n",
            "Episode  69: score -317 (exploration rate: 0.64, transitions: 6964)\n",
            "Episode  70: score -272 (exploration rate: 0.63, transitions: 7065)\n",
            "Episode  71: score -317 (exploration rate: 0.62, transitions: 7166)\n",
            "Episode  72: score -263 (exploration rate: 0.62, transitions: 7267)\n",
            "Episode  73: score -281 (exploration rate: 0.61, transitions: 7368)\n",
            "Episode  74: score -290 (exploration rate: 0.61, transitions: 7469)\n",
            "Episode  75: score -317 (exploration rate: 0.60, transitions: 7570)\n",
            "Episode  76: score -146 (exploration rate: 0.59, transitions: 7671)\n",
            "Episode  77: score -227 (exploration rate: 0.59, transitions: 7772)\n",
            "Episode  78: score -272 (exploration rate: 0.58, transitions: 7873)\n",
            "Episode  79: score -263 (exploration rate: 0.58, transitions: 7974)\n",
            "Episode  80: score -317 (exploration rate: 0.57, transitions: 8075)\n",
            "Episode  81: score -227 (exploration rate: 0.56, transitions: 8176)\n",
            "Episode  82: score -191 (exploration rate: 0.56, transitions: 8277)\n",
            "Episode  83: score -290 (exploration rate: 0.55, transitions: 8378)\n",
            "Episode  84: score -263 (exploration rate: 0.55, transitions: 8479)\n",
            "Episode  85: score -209 (exploration rate: 0.54, transitions: 8580)\n",
            "Episode  86: score -254 (exploration rate: 0.54, transitions: 8681)\n",
            "Episode  87: score -218 (exploration rate: 0.53, transitions: 8782)\n",
            "Episode  88: score -245 (exploration rate: 0.53, transitions: 8883)\n",
            "Episode  89: score -227 (exploration rate: 0.52, transitions: 8984)\n",
            "Episode  90: score -299 (exploration rate: 0.52, transitions: 9085)\n",
            "Episode  91: score -281 (exploration rate: 0.51, transitions: 9186)\n",
            "Episode  92: score -299 (exploration rate: 0.50, transitions: 9287)\n",
            "Episode  93: score -254 (exploration rate: 0.50, transitions: 9388)\n",
            "Episode  94: score -272 (exploration rate: 0.49, transitions: 9489)\n",
            "Episode  95: score -299 (exploration rate: 0.49, transitions: 9590)\n",
            "Episode  96: score -272 (exploration rate: 0.48, transitions: 9691)\n",
            "Episode  97: score -191 (exploration rate: 0.48, transitions: 9792)\n",
            "Episode  98: score -236 (exploration rate: 0.48, transitions: 9893)\n",
            "Episode  99: score -227 (exploration rate: 0.47, transitions: 9994)\n",
            "Episode 100: score -218 (exploration rate: 0.47, transitions: 10095)\n",
            "Episode 101: score -263 (exploration rate: 0.46, transitions: 10196)\n",
            "Episode 102: score -218 (exploration rate: 0.46, transitions: 10297)\n",
            "Episode 103: score -272 (exploration rate: 0.45, transitions: 10398)\n",
            "Episode 104: score -236 (exploration rate: 0.45, transitions: 10499)\n",
            "Episode 105: score -209 (exploration rate: 0.44, transitions: 10600)\n",
            "Episode 106: score -200 (exploration rate: 0.44, transitions: 10701)\n",
            "Episode 107: score -173 (exploration rate: 0.43, transitions: 10802)\n",
            "Episode 108: score -191 (exploration rate: 0.43, transitions: 10903)\n",
            "Episode 109: score -236 (exploration rate: 0.43, transitions: 11004)\n",
            "Episode 110: score -236 (exploration rate: 0.42, transitions: 11105)\n",
            "Episode 111: score -227 (exploration rate: 0.42, transitions: 11206)\n",
            "Episode 112: score -245 (exploration rate: 0.41, transitions: 11307)\n",
            "Episode 113: score -191 (exploration rate: 0.41, transitions: 11408)\n",
            "Episode 114: score -164 (exploration rate: 0.40, transitions: 11509)\n",
            "Episode 115: score -236 (exploration rate: 0.40, transitions: 11610)\n",
            "Episode 116: score -254 (exploration rate: 0.40, transitions: 11711)\n",
            "Episode 117: score -245 (exploration rate: 0.39, transitions: 11812)\n",
            "Episode 118: score -191 (exploration rate: 0.39, transitions: 11913)\n",
            "Episode 119: score -191 (exploration rate: 0.38, transitions: 12014)\n",
            "Episode 120: score -236 (exploration rate: 0.38, transitions: 12115)\n",
            "Episode 121: score -173 (exploration rate: 0.38, transitions: 12216)\n",
            "Episode 122: score -236 (exploration rate: 0.37, transitions: 12317)\n",
            "Episode 123: score -227 (exploration rate: 0.37, transitions: 12418)\n",
            "Episode 124: score -200 (exploration rate: 0.37, transitions: 12519)\n",
            "Episode 125: score -200 (exploration rate: 0.36, transitions: 12620)\n",
            "Episode 126: score -182 (exploration rate: 0.36, transitions: 12721)\n",
            "Episode 127: score -227 (exploration rate: 0.36, transitions: 12822)\n",
            "Episode 128: score -218 (exploration rate: 0.35, transitions: 12923)\n",
            "Episode 129: score -155 (exploration rate: 0.35, transitions: 13024)\n",
            "Episode 130: score -236 (exploration rate: 0.34, transitions: 13125)\n",
            "Episode 131: score -200 (exploration rate: 0.34, transitions: 13226)\n",
            "Episode 132: score -191 (exploration rate: 0.34, transitions: 13327)\n",
            "Episode 133: score -218 (exploration rate: 0.33, transitions: 13428)\n",
            "Episode 134: score -209 (exploration rate: 0.33, transitions: 13529)\n",
            "Episode 135: score -173 (exploration rate: 0.33, transitions: 13630)\n",
            "Episode 136: score -182 (exploration rate: 0.32, transitions: 13731)\n",
            "Episode 137: score -137 (exploration rate: 0.32, transitions: 13832)\n",
            "Episode 138: score -182 (exploration rate: 0.32, transitions: 13933)\n",
            "Episode 139: score -191 (exploration rate: 0.31, transitions: 14034)\n",
            "Episode 140: score -191 (exploration rate: 0.31, transitions: 14135)\n",
            "Episode 141: score -200 (exploration rate: 0.31, transitions: 14236)\n",
            "Episode 142: score -173 (exploration rate: 0.31, transitions: 14337)\n",
            "Episode 143: score -227 (exploration rate: 0.30, transitions: 14438)\n",
            "Episode 144: score -254 (exploration rate: 0.30, transitions: 14539)\n",
            "Episode 145: score -182 (exploration rate: 0.30, transitions: 14640)\n",
            "Episode 146: score -209 (exploration rate: 0.29, transitions: 14741)\n",
            "Episode 147: score -164 (exploration rate: 0.29, transitions: 14842)\n",
            "Episode 148: score -173 (exploration rate: 0.29, transitions: 14943)\n",
            "Episode 149: score -200 (exploration rate: 0.28, transitions: 15044)\n",
            "Episode 150: score -182 (exploration rate: 0.28, transitions: 15145)\n",
            "Episode 151: score -182 (exploration rate: 0.28, transitions: 15246)\n",
            "Episode 152: score -200 (exploration rate: 0.28, transitions: 15347)\n",
            "Episode 153: score -209 (exploration rate: 0.27, transitions: 15448)\n",
            "Episode 154: score -137 (exploration rate: 0.27, transitions: 15549)\n",
            "Episode 155: score -236 (exploration rate: 0.27, transitions: 15650)\n",
            "Episode 156: score -191 (exploration rate: 0.27, transitions: 15751)\n",
            "Episode 157: score -155 (exploration rate: 0.26, transitions: 15852)\n",
            "Episode 158: score -209 (exploration rate: 0.26, transitions: 15953)\n",
            "Episode 159: score -164 (exploration rate: 0.26, transitions: 16054)\n",
            "Episode 160: score -164 (exploration rate: 0.25, transitions: 16155)\n",
            "Episode 161: score -191 (exploration rate: 0.25, transitions: 16256)\n",
            "Episode 162: score -155 (exploration rate: 0.25, transitions: 16357)\n",
            "Episode 163: score -182 (exploration rate: 0.25, transitions: 16458)\n",
            "Episode 164: score -209 (exploration rate: 0.24, transitions: 16559)\n",
            "Episode 165: score -146 (exploration rate: 0.24, transitions: 16660)\n",
            "Episode 166: score -146 (exploration rate: 0.24, transitions: 16761)\n",
            "Episode 167: score -218 (exploration rate: 0.24, transitions: 16862)\n",
            "Episode 168: score -191 (exploration rate: 0.24, transitions: 16963)\n",
            "Episode 169: score -173 (exploration rate: 0.23, transitions: 17064)\n",
            "Episode 170: score -146 (exploration rate: 0.23, transitions: 17165)\n",
            "Episode 171: score -191 (exploration rate: 0.23, transitions: 17266)\n",
            "Episode 172: score -137 (exploration rate: 0.23, transitions: 17367)\n",
            "Episode 173: score -137 (exploration rate: 0.22, transitions: 17468)\n",
            "Episode 174: score -146 (exploration rate: 0.22, transitions: 17569)\n",
            "Episode 175: score -164 (exploration rate: 0.22, transitions: 17670)\n",
            "Episode 176: score -173 (exploration rate: 0.22, transitions: 17771)\n",
            "Episode 177: score -164 (exploration rate: 0.21, transitions: 17872)\n",
            "Episode 178: score -164 (exploration rate: 0.21, transitions: 17973)\n",
            "Episode 179: score -191 (exploration rate: 0.21, transitions: 18074)\n",
            "Episode 180: score -182 (exploration rate: 0.21, transitions: 18175)\n",
            "Episode 181: score -182 (exploration rate: 0.21, transitions: 18276)\n",
            "Episode 182: score -173 (exploration rate: 0.20, transitions: 18377)\n",
            "Episode 183: score -191 (exploration rate: 0.20, transitions: 18478)\n",
            "Episode 184: score -128 (exploration rate: 0.20, transitions: 18579)\n",
            "Episode 185: score -191 (exploration rate: 0.20, transitions: 18680)\n",
            "Episode 186: score -155 (exploration rate: 0.20, transitions: 18781)\n",
            "Episode 187: score -128 (exploration rate: 0.19, transitions: 18882)\n",
            "Episode 188: score -128 (exploration rate: 0.19, transitions: 18983)\n",
            "Episode 189: score -146 (exploration rate: 0.19, transitions: 19084)\n",
            "Episode 190: score -182 (exploration rate: 0.19, transitions: 19185)\n",
            "Episode 191: score -146 (exploration rate: 0.19, transitions: 19286)\n",
            "Episode 192: score -137 (exploration rate: 0.18, transitions: 19387)\n",
            "Episode 193: score -137 (exploration rate: 0.18, transitions: 19488)\n",
            "Episode 194: score -164 (exploration rate: 0.18, transitions: 19589)\n",
            "Episode 195: score -164 (exploration rate: 0.18, transitions: 19690)\n",
            "Episode 196: score -137 (exploration rate: 0.18, transitions: 19791)\n",
            "Episode 197: score -164 (exploration rate: 0.18, transitions: 19892)\n",
            "Episode 198: score -146 (exploration rate: 0.17, transitions: 19993)\n",
            "Episode 199: score -137 (exploration rate: 0.17, transitions: 20094)\n",
            "Episode 200: score -137 (exploration rate: 0.17, transitions: 20195)\n",
            "Episode 201: score -128 (exploration rate: 0.17, transitions: 20296)\n",
            "Episode 202: score -119 (exploration rate: 0.17, transitions: 20397)\n",
            "Episode 203: score -137 (exploration rate: 0.17, transitions: 20498)\n",
            "Episode 204: score -164 (exploration rate: 0.16, transitions: 20599)\n",
            "Episode 205: score -146 (exploration rate: 0.16, transitions: 20700)\n",
            "Episode 206: score -128 (exploration rate: 0.16, transitions: 20801)\n",
            "Episode 207: score -128 (exploration rate: 0.16, transitions: 20902)\n",
            "Episode 208: score -164 (exploration rate: 0.16, transitions: 21003)\n",
            "Episode 209: score -173 (exploration rate: 0.16, transitions: 21104)\n",
            "Episode 210: score -128 (exploration rate: 0.15, transitions: 21205)\n",
            "Episode 211: score -146 (exploration rate: 0.15, transitions: 21306)\n",
            "Episode 212: score -137 (exploration rate: 0.15, transitions: 21407)\n",
            "Episode 213: score -155 (exploration rate: 0.15, transitions: 21508)\n",
            "Episode 214: score -155 (exploration rate: 0.15, transitions: 21609)\n",
            "Episode 215: score -137 (exploration rate: 0.15, transitions: 21710)\n",
            "Episode 216: score -137 (exploration rate: 0.15, transitions: 21811)\n",
            "Episode 217: score -146 (exploration rate: 0.14, transitions: 21912)\n",
            "Episode 218: score -119 (exploration rate: 0.14, transitions: 22013)\n",
            "Episode 219: score -146 (exploration rate: 0.14, transitions: 22114)\n",
            "Episode 220: score -164 (exploration rate: 0.14, transitions: 22215)\n",
            "Episode 221: score -137 (exploration rate: 0.14, transitions: 22316)\n",
            "Episode 222: score -128 (exploration rate: 0.14, transitions: 22417)\n",
            "Episode 223: score -119 (exploration rate: 0.14, transitions: 22518)\n",
            "Episode 224: score -128 (exploration rate: 0.13, transitions: 22619)\n",
            "Episode 225: score -119 (exploration rate: 0.13, transitions: 22720)\n",
            "Episode 226: score -137 (exploration rate: 0.13, transitions: 22821)\n",
            "Episode 227: score -164 (exploration rate: 0.13, transitions: 22922)\n",
            "Episode 228: score -119 (exploration rate: 0.13, transitions: 23023)\n",
            "Episode 229: score -110 (exploration rate: 0.13, transitions: 23124)\n",
            "Episode 230: score -128 (exploration rate: 0.13, transitions: 23225)\n",
            "Episode 231: score -137 (exploration rate: 0.12, transitions: 23326)\n",
            "Episode 232: score -128 (exploration rate: 0.12, transitions: 23427)\n",
            "Episode 233: score -128 (exploration rate: 0.12, transitions: 23528)\n",
            "Episode 234: score -110 (exploration rate: 0.12, transitions: 23629)\n",
            "Episode 235: score -137 (exploration rate: 0.12, transitions: 23730)\n",
            "Episode 236: score -137 (exploration rate: 0.12, transitions: 23831)\n",
            "Episode 237: score -128 (exploration rate: 0.12, transitions: 23932)\n",
            "Episode 238: score -173 (exploration rate: 0.12, transitions: 24033)\n",
            "Episode 239: score -173 (exploration rate: 0.12, transitions: 24134)\n",
            "Episode 240: score -110 (exploration rate: 0.11, transitions: 24235)\n",
            "Episode 241: score -119 (exploration rate: 0.11, transitions: 24336)\n",
            "Episode 242: score -128 (exploration rate: 0.11, transitions: 24437)\n",
            "Episode 243: score -128 (exploration rate: 0.11, transitions: 24538)\n",
            "Episode 244: score -155 (exploration rate: 0.11, transitions: 24639)\n",
            "Episode 245: score -137 (exploration rate: 0.11, transitions: 24740)\n",
            "Episode 246: score -119 (exploration rate: 0.11, transitions: 24841)\n",
            "Episode 247: score -137 (exploration rate: 0.11, transitions: 24942)\n",
            "Episode 248: score -137 (exploration rate: 0.11, transitions: 25043)\n",
            "Episode 249: score -137 (exploration rate: 0.10, transitions: 25144)\n",
            "Episode 250: score -137 (exploration rate: 0.10, transitions: 25245)\n",
            "Episode 251: score -164 (exploration rate: 0.10, transitions: 25346)\n",
            "Episode 252: score -119 (exploration rate: 0.10, transitions: 25447)\n",
            "Episode 253: score -173 (exploration rate: 0.10, transitions: 25548)\n",
            "Episode 254: score -137 (exploration rate: 0.10, transitions: 25649)\n",
            "Episode 255: score -137 (exploration rate: 0.10, transitions: 25750)\n",
            "Episode 256: score -137 (exploration rate: 0.10, transitions: 25851)\n",
            "Episode 257: score -155 (exploration rate: 0.10, transitions: 25952)\n",
            "Episode 258: score -137 (exploration rate: 0.10, transitions: 26053)\n",
            "Episode 259: score -119 (exploration rate: 0.09, transitions: 26154)\n",
            "Episode 260: score -164 (exploration rate: 0.09, transitions: 26255)\n",
            "Episode 261: score -128 (exploration rate: 0.09, transitions: 26356)\n",
            "Episode 262: score -128 (exploration rate: 0.09, transitions: 26457)\n",
            "Episode 263: score -110 (exploration rate: 0.09, transitions: 26558)\n",
            "Episode 264: score -119 (exploration rate: 0.09, transitions: 26659)\n",
            "Episode 265: score -119 (exploration rate: 0.09, transitions: 26760)\n",
            "Episode 266: score -119 (exploration rate: 0.09, transitions: 26861)\n",
            "Episode 267: score -137 (exploration rate: 0.09, transitions: 26962)\n",
            "Episode 268: score -101 (exploration rate: 0.09, transitions: 27063)\n",
            "Episode 269: score -137 (exploration rate: 0.09, transitions: 27164)\n",
            "Episode 270: score -146 (exploration rate: 0.08, transitions: 27265)\n",
            "Episode 271: score -119 (exploration rate: 0.08, transitions: 27366)\n",
            "Episode 272: score -110 (exploration rate: 0.08, transitions: 27467)\n",
            "Episode 273: score -128 (exploration rate: 0.08, transitions: 27568)\n",
            "Episode 274: score -119 (exploration rate: 0.08, transitions: 27669)\n",
            "Episode 275: score -137 (exploration rate: 0.08, transitions: 27770)\n",
            "Episode 276: score -128 (exploration rate: 0.08, transitions: 27871)\n",
            "Episode 277: score -137 (exploration rate: 0.08, transitions: 27972)\n",
            "Episode 278: score -119 (exploration rate: 0.08, transitions: 28073)\n",
            "Episode 279: score -128 (exploration rate: 0.08, transitions: 28174)\n",
            "Episode 280: score -119 (exploration rate: 0.08, transitions: 28275)\n",
            "Episode 281: score -119 (exploration rate: 0.08, transitions: 28376)\n",
            "Episode 282: score -110 (exploration rate: 0.07, transitions: 28477)\n",
            "Episode 283: score -137 (exploration rate: 0.07, transitions: 28578)\n",
            "Episode 284: score -110 (exploration rate: 0.07, transitions: 28679)\n",
            "Episode 285: score -128 (exploration rate: 0.07, transitions: 28780)\n",
            "Episode 286: score -146 (exploration rate: 0.07, transitions: 28881)\n",
            "Episode 287: score -119 (exploration rate: 0.07, transitions: 28982)\n",
            "Episode 288: score -119 (exploration rate: 0.07, transitions: 29083)\n",
            "Episode 289: score -110 (exploration rate: 0.07, transitions: 29184)\n",
            "Episode 290: score -128 (exploration rate: 0.07, transitions: 29285)\n",
            "Episode 291: score -128 (exploration rate: 0.07, transitions: 29386)\n",
            "Episode 292: score -128 (exploration rate: 0.07, transitions: 29487)\n",
            "Episode 293: score -110 (exploration rate: 0.07, transitions: 29588)\n",
            "Episode 294: score -128 (exploration rate: 0.07, transitions: 29689)\n",
            "Episode 295: score -110 (exploration rate: 0.07, transitions: 29790)\n",
            "Episode 296: score -110 (exploration rate: 0.06, transitions: 29891)\n",
            "Episode 297: score -119 (exploration rate: 0.06, transitions: 29992)\n",
            "Episode 298: score -119 (exploration rate: 0.06, transitions: 30093)\n",
            "Episode 299: score -101 (exploration rate: 0.06, transitions: 30194)\n",
            "Episode 300: score -137 (exploration rate: 0.06, transitions: 30295)\n",
            "Episode 301: score -110 (exploration rate: 0.06, transitions: 30396)\n",
            "Episode 302: score -101 (exploration rate: 0.06, transitions: 30497)\n",
            "Episode 303: score -128 (exploration rate: 0.06, transitions: 30598)\n",
            "Episode 304: score -119 (exploration rate: 0.06, transitions: 30699)\n",
            "Episode 305: score -128 (exploration rate: 0.06, transitions: 30800)\n",
            "Episode 306: score -110 (exploration rate: 0.06, transitions: 30901)\n",
            "Episode 307: score -119 (exploration rate: 0.06, transitions: 31002)\n",
            "Episode 308: score -110 (exploration rate: 0.06, transitions: 31103)\n",
            "Episode 309: score -119 (exploration rate: 0.06, transitions: 31204)\n",
            "Episode 310: score -119 (exploration rate: 0.06, transitions: 31305)\n",
            "Episode 311: score -101 (exploration rate: 0.06, transitions: 31406)\n",
            "Episode 312: score -128 (exploration rate: 0.06, transitions: 31507)\n",
            "Episode 313: score -128 (exploration rate: 0.05, transitions: 31608)\n",
            "Episode 314: score -110 (exploration rate: 0.05, transitions: 31709)\n",
            "Episode 315: score -137 (exploration rate: 0.05, transitions: 31810)\n",
            "Episode 316: score -119 (exploration rate: 0.05, transitions: 31911)\n",
            "Episode 317: score -110 (exploration rate: 0.05, transitions: 32012)\n",
            "Episode 318: score -110 (exploration rate: 0.05, transitions: 32113)\n",
            "Episode 319: score -110 (exploration rate: 0.05, transitions: 32214)\n",
            "Episode 320: score -119 (exploration rate: 0.05, transitions: 32315)\n",
            "Episode 321: score -119 (exploration rate: 0.05, transitions: 32416)\n",
            "Episode 322: score -119 (exploration rate: 0.05, transitions: 32517)\n",
            "Episode 323: score -119 (exploration rate: 0.05, transitions: 32618)\n",
            "Episode 324: score -119 (exploration rate: 0.05, transitions: 32719)\n",
            "Episode 325: score -119 (exploration rate: 0.05, transitions: 32820)\n",
            "Episode 326: score -128 (exploration rate: 0.05, transitions: 32921)\n",
            "Episode 327: score -119 (exploration rate: 0.05, transitions: 33022)\n",
            "Episode 328: score -101 (exploration rate: 0.05, transitions: 33123)\n",
            "Episode 329: score -119 (exploration rate: 0.05, transitions: 33224)\n",
            "Episode 330: score -110 (exploration rate: 0.05, transitions: 33325)\n",
            "Episode 331: score -128 (exploration rate: 0.05, transitions: 33426)\n",
            "Episode 332: score -128 (exploration rate: 0.05, transitions: 33527)\n",
            "Episode 333: score -128 (exploration rate: 0.05, transitions: 33628)\n",
            "Episode 334: score -101 (exploration rate: 0.05, transitions: 33729)\n",
            "Episode 335: score -110 (exploration rate: 0.05, transitions: 33830)\n",
            "Episode 336: score -128 (exploration rate: 0.05, transitions: 33931)\n",
            "Episode 337: score -110 (exploration rate: 0.05, transitions: 34032)\n",
            "Episode 338: score -119 (exploration rate: 0.05, transitions: 34133)\n",
            "Episode 339: score -119 (exploration rate: 0.05, transitions: 34234)\n",
            "Episode 340: score -110 (exploration rate: 0.05, transitions: 34335)\n",
            "Episode 341: score -110 (exploration rate: 0.05, transitions: 34436)\n",
            "Episode 342: score -110 (exploration rate: 0.05, transitions: 34537)\n",
            "Episode 343: score -101 (exploration rate: 0.05, transitions: 34638)\n",
            "Episode 344: score -110 (exploration rate: 0.05, transitions: 34739)\n",
            "Episode 345: score -128 (exploration rate: 0.05, transitions: 34840)\n",
            "Episode 346: score -110 (exploration rate: 0.05, transitions: 34941)\n",
            "Episode 347: score -110 (exploration rate: 0.05, transitions: 35042)\n",
            "Episode 348: score -119 (exploration rate: 0.05, transitions: 35143)\n",
            "Episode 349: score -101 (exploration rate: 0.05, transitions: 35244)\n",
            "Episode 350: score -110 (exploration rate: 0.05, transitions: 35345)\n",
            "Episode 351: score -110 (exploration rate: 0.05, transitions: 35446)\n",
            "Episode 352: score -101 (exploration rate: 0.05, transitions: 35547)\n",
            "Episode 353: score -119 (exploration rate: 0.05, transitions: 35648)\n",
            "Episode 354: score -101 (exploration rate: 0.05, transitions: 35749)\n",
            "Episode 355: score -119 (exploration rate: 0.05, transitions: 35850)\n",
            "Episode 356: score -119 (exploration rate: 0.05, transitions: 35951)\n",
            "Episode 357: score -101 (exploration rate: 0.05, transitions: 36052)\n",
            "Episode 358: score -110 (exploration rate: 0.05, transitions: 36153)\n",
            "Episode 359: score -110 (exploration rate: 0.05, transitions: 36254)\n",
            "Episode 360: score -110 (exploration rate: 0.05, transitions: 36355)\n",
            "Episode 361: score -119 (exploration rate: 0.05, transitions: 36456)\n",
            "Episode 362: score -110 (exploration rate: 0.05, transitions: 36557)\n",
            "Episode 363: score -128 (exploration rate: 0.05, transitions: 36658)\n",
            "Episode 364: score -101 (exploration rate: 0.05, transitions: 36759)\n",
            "Episode 365: score -128 (exploration rate: 0.05, transitions: 36860)\n",
            "Episode 366: score -110 (exploration rate: 0.05, transitions: 36961)\n",
            "Episode 367: score -128 (exploration rate: 0.05, transitions: 37062)\n",
            "Episode 368: score -101 (exploration rate: 0.05, transitions: 37163)\n",
            "Episode 369: score -137 (exploration rate: 0.05, transitions: 37264)\n",
            "Episode 370: score -101 (exploration rate: 0.05, transitions: 37365)\n",
            "Episode 371: score -137 (exploration rate: 0.05, transitions: 37466)\n",
            "Episode 372: score -101 (exploration rate: 0.05, transitions: 37567)\n",
            "Episode 373: score -119 (exploration rate: 0.05, transitions: 37668)\n",
            "Episode 374: score -110 (exploration rate: 0.05, transitions: 37769)\n",
            "Episode 375: score -110 (exploration rate: 0.05, transitions: 37870)\n",
            "Episode 376: score -128 (exploration rate: 0.05, transitions: 37971)\n",
            "Episode 377: score -110 (exploration rate: 0.05, transitions: 38072)\n",
            "Episode 378: score -119 (exploration rate: 0.05, transitions: 38173)\n",
            "Episode 379: score -128 (exploration rate: 0.05, transitions: 38274)\n",
            "Episode 380: score -110 (exploration rate: 0.05, transitions: 38375)\n",
            "Episode 381: score -101 (exploration rate: 0.05, transitions: 38476)\n",
            "Episode 382: score -119 (exploration rate: 0.05, transitions: 38577)\n",
            "Episode 383: score -119 (exploration rate: 0.05, transitions: 38678)\n",
            "Episode 384: score -101 (exploration rate: 0.05, transitions: 38779)\n",
            "Episode 385: score -119 (exploration rate: 0.05, transitions: 38880)\n",
            "Episode 386: score -110 (exploration rate: 0.05, transitions: 38981)\n",
            "Episode 387: score -110 (exploration rate: 0.05, transitions: 39082)\n",
            "Episode 388: score -110 (exploration rate: 0.05, transitions: 39183)\n",
            "Episode 389: score -119 (exploration rate: 0.05, transitions: 39284)\n",
            "Episode 390: score -128 (exploration rate: 0.05, transitions: 39385)\n",
            "Episode 391: score -146 (exploration rate: 0.05, transitions: 39486)\n",
            "Episode 392: score -128 (exploration rate: 0.05, transitions: 39587)\n",
            "Episode 393: score -101 (exploration rate: 0.05, transitions: 39688)\n",
            "Episode 394: score -110 (exploration rate: 0.05, transitions: 39789)\n",
            "Episode 395: score -101 (exploration rate: 0.05, transitions: 39890)\n",
            "Episode 396: score -101 (exploration rate: 0.05, transitions: 39991)\n",
            "Episode 397: score -110 (exploration rate: 0.05, transitions: 40092)\n",
            "Episode 398: score -110 (exploration rate: 0.05, transitions: 40193)\n",
            "Episode 399: score -110 (exploration rate: 0.05, transitions: 40294)\n",
            "Episode 400: score -101 (exploration rate: 0.05, transitions: 40395)\n",
            "Episode 401: score -110 (exploration rate: 0.05, transitions: 40496)\n",
            "Episode 402: score -101 (exploration rate: 0.05, transitions: 40597)\n",
            "Episode 403: score -128 (exploration rate: 0.05, transitions: 40698)\n",
            "Episode 404: score -101 (exploration rate: 0.05, transitions: 40799)\n",
            "Episode 405: score -128 (exploration rate: 0.05, transitions: 40900)\n",
            "Episode 406: score -110 (exploration rate: 0.05, transitions: 41001)\n",
            "Episode 407: score -119 (exploration rate: 0.05, transitions: 41102)\n",
            "Episode 408: score -128 (exploration rate: 0.05, transitions: 41203)\n",
            "Episode 409: score -119 (exploration rate: 0.05, transitions: 41304)\n",
            "Episode 410: score -110 (exploration rate: 0.05, transitions: 41405)\n",
            "Episode 411: score -101 (exploration rate: 0.05, transitions: 41506)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[113], line 53\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore (average last 10 episodes):\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_score)\n\u001b[0;32m     50\u001b[0m     agent\u001b[38;5;241m.\u001b[39mdisplay_scores_graphically()\n\u001b[1;32m---> 53\u001b[0m \u001b[43mborrar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[113], line 29\u001b[0m, in \u001b[0;36mborrar\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Learn using a batch of experience stored in memory\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Detect end of episode\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminal_state \u001b[38;5;129;01mor\u001b[39;00m truncated \u001b[38;5;129;01mor\u001b[39;00m steps\u001b[38;5;241m>\u001b[39mMAX_STEPS_PER_EPISODE:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# TODO: pendiente de comprobar\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[111], line 55\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m states, actions, rewards, next_states, terminal_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample_memory(BATCH_SIZE)\n\u001b[0;32m     54\u001b[0m q_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(states, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m q_next_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BATCH_SIZE):\n\u001b[0;32m     58\u001b[0m      \u001b[38;5;28;01mif\u001b[39;00m (terminal_states[i]):\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:2620\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2611\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2612\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2614\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2617\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2618\u001b[0m         )\n\u001b[1;32m-> 2620\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1291\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:355\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:407\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuffle:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# See b/141490660 for more details.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     options\u001b[38;5;241m.\u001b[39mexperimental_external_state_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    405\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mExternalStatePolicy\u001b[38;5;241m.\u001b[39mIGNORE\n\u001b[0;32m    406\u001b[0m     )\n\u001b[1;32m--> 407\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2986\u001b[0m, in \u001b[0;36mDatasetV2.with_options\u001b[1;34m(self, options, name)\u001b[0m\n\u001b[0;32m   2960\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_options\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetV2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2961\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new `tf.data.Dataset` with the given options set.\u001b[39;00m\n\u001b[0;32m   2962\u001b[0m \n\u001b[0;32m   2963\u001b[0m \u001b[38;5;124;03m  The options are \"global\" in the sense they apply to the entire dataset.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[38;5;124;03m    ValueError: when an option is set more than once to a non-default value\u001b[39;00m\n\u001b[0;32m   2985\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2986\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_OptionsDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4852\u001b[0m, in \u001b[0;36m_OptionsDataset.__init__\u001b[1;34m(self, input_dataset, options, name)\u001b[0m\n\u001b[0;32m   4850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   4851\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m-> 4852\u001b[0m   variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39moptions_dataset(\n\u001b[0;32m   4853\u001b[0m       input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor, options_pb\u001b[38;5;241m.\u001b[39mSerializeToString(),\n\u001b[0;32m   4854\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m   4855\u001b[0m \u001b[38;5;28msuper\u001b[39m(_OptionsDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n\u001b[0;32m   4857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options_attr:\n",
            "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:4595\u001b[0m, in \u001b[0;36moptions_dataset\u001b[1;34m(input_dataset, serialized_options, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[0;32m   4593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   4594\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4595\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4596\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionsDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserialized_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4597\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4598\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   4600\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def borrar():\n",
        "    environment, number_of_observations, number_of_actions = create_environment()\n",
        "    agent = DQN(number_of_observations, number_of_actions)\n",
        "    episode = 0\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    while (episode < NUMBER_OF_EPISODES_FOR_TRAINING):\n",
        "        episode += 1\n",
        "        score = 0\n",
        "        state, info = environment.reset()\n",
        "        end_episode = False\n",
        "        steps = 0\n",
        "        agent.decrease_exploration_rate(episode)\n",
        "        \n",
        "        while not(end_episode):\n",
        "            steps += 1\n",
        "            # Select an action for the current state\n",
        "            action = agent.select(state)\n",
        "\n",
        "            # Execute the action on the environment\n",
        "            state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
        "        \n",
        "            # Store in memory the transition (s,a,r,s')\n",
        "            agent.remember(state, action, reward, state_next, terminal_state)\n",
        "\n",
        "            score += reward\n",
        "\n",
        "            # Learn using a batch of experience stored in memory\n",
        "            agent.learn()\n",
        "\n",
        "\n",
        "            # Detect end of episode\n",
        "            if terminal_state or truncated or steps>MAX_STEPS_PER_EPISODE:\n",
        "                # TODO: pendiente de comprobar\n",
        "                agent.add_score(score)\n",
        "                print(\"Episode {0:>3}: \".format(episode), end = '')\n",
        "                print(\"score {0:>3} \".format(math.trunc(score)), end = '')\n",
        "                print(\"(exploration rate: %.2f, \" % agent.exploration_rate, end = '')\n",
        "                print(\"transitions: \" + str(agent.memory.current_size) + \")\")\n",
        "                end_episode = True\n",
        "            else:\n",
        "                state = state_next\n",
        "\n",
        "\n",
        "    print(\"Time for training:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n",
        "    print(\"Score (max):\", max(agent.scores))\n",
        "    average_score = np.mean(agent.scores[max(0,(len(agent.scores)-10)):(len(agent.scores))])\n",
        "    print(\"Score (average last 10 episodes):\", average_score)\n",
        "\n",
        "    agent.display_scores_graphically()\n",
        "\n",
        "\n",
        "borrar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSteJT6ULQVG"
      },
      "source": [
        "\n",
        "## Testing program\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIP0LQHGLZPj",
        "outputId": "906f931f-d01b-4147-f802-e3ea45f44045"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # TODO: aún nada hecho\n",
        "\n",
        "    agent.delete_scores()\n",
        "    episode = 0\n",
        "    start_time = time.perf_counter()\n",
        "    while (episode < NUMBER_OF_EPISODES_FOR_TESTING):\n",
        "        episode += 1\n",
        "        score = 0\n",
        "        state, info = environment.reset()\n",
        "        end_episode = False\n",
        "        while not(end_episode):\n",
        "            # Select an action for the current state\n",
        "            action = agent.select_greedy_policy(state)\n",
        "\n",
        "            # Execute the action in the environment\n",
        "            state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
        "\n",
        "            score += reward\n",
        "\n",
        "            # Detect end of episode and print\n",
        "            if terminal_state or truncated:\n",
        "                agent.add_score(score)\n",
        "                print(\"Episode {0:>3}: \".format(episode), end = '')\n",
        "                print(\"score {0:>3} \\n\".format(math.trunc(score)), end = '')\n",
        "                end_episode = True\n",
        "            else:\n",
        "                state = state_next\n",
        "\n",
        "    print(\"Time for testing:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n",
        "    print(\"Score (average):\", np.mean(agent.scores))\n",
        "    print(\"Score (max):\", max(agent.scores))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
