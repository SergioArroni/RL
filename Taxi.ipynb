{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e7ace4-da89-4413-bd44-dbb277bc3814",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# imports\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam,SGD\n",
    "from collections import deque\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import gym\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set seed for torch library\n",
    "torch.manual_seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37347cdc-f29b-40c3-9247-407939b03aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<TaxiEnv<Taxi-v3>>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the taxi environment and verify\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1d66d-be13-4a36-bdd0-814047c1ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(500)\n",
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# view observation space and action space\n",
    "obs_space = env.observation_space\n",
    "print(f\"Observation space: {obs_space}\")\n",
    "act_space = env.action_space\n",
    "print(f\"Action space: {act_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af066590-67b8-4363-8533-71a2b69c3ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: 92\n",
      "Action: 1\n",
      "New observation: (-1, False, False)\n",
      "Exp: [92, -1, False, False]\n"
     ]
    }
   ],
   "source": [
    "# test random environment, action, and step\n",
    "obs_test = env.reset()\n",
    "act_test = env.action_space.sample()\n",
    "obs_test, nst, rt, dt, _ = env.step(act_test)\n",
    "print(\n",
    "    f\"Initial observation: {obs_test}\\nAction: {act_test}\\nNew observation: {nst,rt,dt}\"\n",
    ")\n",
    "exp_test = [obs_test, nst, rt, dt]\n",
    "print(f\"Exp: {exp_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb5602-704c-4e71-a478-77bea80256c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Q for state 2: 22.0\n",
      "Corresponding action: 3\n",
      "Q:\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  2. 10. 22.  5.  3.]\n",
      " ...\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "Shape of Q: (500, 6)\n",
      "Q[2,3] = 5.0\n",
      "[ 1.  2. 10. 22.  5.  3.]\n"
     ]
    }
   ],
   "source": [
    "# create test q-table with initial values as 0 to preview and test q action\n",
    "Q_test = np.zeros((500,6)) # environment has 500 states and 6 actions\n",
    "# Q_test[state][action]\n",
    "state_test = 2\n",
    "Q_test[state_test] = [1,2,10,22,5,3]\n",
    "a_test = np.argmax(Q_test[state_test])\n",
    "print(f\"Max Q for state {state_test}: {np.max(Q_test[state_test])}\")\n",
    "print(f\"Corresponding action: {a_test}\")\n",
    "print(f\"Q:\\n{Q_test}\\nShape of Q: {Q_test.shape}\")\n",
    "print(f\"Q[2,3] = {Q_test[2,4]}\")\n",
    "print(Q_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991f4a5-3d31-4dce-94d6-3b8994327491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# test random action\n",
    "for i in range(10):\n",
    "    test_a = env.action_space.sample()\n",
    "    print(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d32ce-4b74-4ba7-b253-c938fa897f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "Q shape: (500, 6)\n"
     ]
    }
   ],
   "source": [
    "# create actual Q table\n",
    "Q = np.zeros((500,6)) # environment has 500 states and 6 actions\n",
    "print(f\"Q:\\n{Q}\\nQ shape: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7dbb1b-13e7-41c8-94ce-1974718fa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore / exploit\n",
    "\n",
    "def action(state,epsilon=0.5):\n",
    "    \n",
    "    if random.uniform(0,1) < epsilon: # choose random action if random number is less than epsilon\n",
    "        a1 = env.action_space.sample()\n",
    "        return a1\n",
    "    \n",
    "    else: # choose action with max Q value if random number is greater than epsilon\n",
    "        if np.max(Q[state]) > 0: # choose action with max Q value if max Q for state is greater than 0\n",
    "            a2 = np.argmax(Q[state]) # action corresponding to max Q for that state\n",
    "            return a2\n",
    "        else: # otherwise random action\n",
    "            a3 = env.action_space.sample()\n",
    "            return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33e72-0459-4393-8413-85dbb1a32150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "\n",
    "def q_learning(episodes,lr,gamma):\n",
    "    #print(state,action)\n",
    "    for i in range(episodes):\n",
    "        # if i%1000 == 0:\n",
    "        #     print(f\"Episode {i}:\")\n",
    "        obs = env.reset()\n",
    "        d = False\n",
    "        while d == False:\n",
    "            act = action(obs)\n",
    "            ns,r,d,_=env.step(act)\n",
    "            exp = [obs,ns,r,d]\n",
    "            #print(f\"exp: {exp}\")\n",
    "            Q[obs,act] = Q[obs,act] + lr * (exp[2] + gamma * np.max(Q[ns,:]) - Q[obs,act]) # bellman eqn\n",
    "            obs = ns\n",
    "        if i%1000 == 0:\n",
    "            print(f\"\\tend of episode {i}\")\n",
    "    print(f\"\\nEND OF ALL EPISODES\\n\\nFinal Q-values:\\n{Q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a09f90-6ed4-4ae4-917d-e969e92f45a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting learning:\n",
      "\tend of episode 0\n",
      "\tend of episode 1000\n",
      "\tend of episode 2000\n",
      "\tend of episode 3000\n",
      "\tend of episode 4000\n",
      "\tend of episode 5000\n",
      "\tend of episode 6000\n",
      "\tend of episode 7000\n",
      "\tend of episode 8000\n",
      "\tend of episode 9000\n",
      "\tend of episode 10000\n",
      "\tend of episode 11000\n",
      "\tend of episode 12000\n",
      "\tend of episode 13000\n",
      "\tend of episode 14000\n",
      "\tend of episode 15000\n",
      "\tend of episode 16000\n",
      "\tend of episode 17000\n",
      "\tend of episode 18000\n",
      "\tend of episode 19000\n",
      "\tend of episode 20000\n",
      "\tend of episode 21000\n",
      "\tend of episode 22000\n",
      "\tend of episode 23000\n",
      "\tend of episode 24000\n",
      "\tend of episode 25000\n",
      "\tend of episode 26000\n",
      "\tend of episode 27000\n",
      "\tend of episode 28000\n",
      "\tend of episode 29000\n",
      "\tend of episode 30000\n",
      "\tend of episode 31000\n",
      "\tend of episode 32000\n",
      "\tend of episode 33000\n",
      "\tend of episode 34000\n",
      "\tend of episode 35000\n",
      "\tend of episode 36000\n",
      "\tend of episode 37000\n",
      "\tend of episode 38000\n",
      "\tend of episode 39000\n",
      "\tend of episode 40000\n",
      "\tend of episode 41000\n",
      "\tend of episode 42000\n",
      "\tend of episode 43000\n",
      "\tend of episode 44000\n",
      "\tend of episode 45000\n",
      "\tend of episode 46000\n",
      "\tend of episode 47000\n",
      "\tend of episode 48000\n",
      "\tend of episode 49000\n",
      "\tend of episode 50000\n",
      "\tend of episode 51000\n",
      "\tend of episode 52000\n",
      "\tend of episode 53000\n",
      "\tend of episode 54000\n",
      "\tend of episode 55000\n",
      "\tend of episode 56000\n",
      "\tend of episode 57000\n",
      "\tend of episode 58000\n",
      "\tend of episode 59000\n",
      "\tend of episode 60000\n",
      "\tend of episode 61000\n",
      "\tend of episode 62000\n",
      "\tend of episode 63000\n",
      "\tend of episode 64000\n",
      "\tend of episode 65000\n",
      "\tend of episode 66000\n",
      "\tend of episode 67000\n",
      "\tend of episode 68000\n",
      "\tend of episode 69000\n",
      "\tend of episode 70000\n",
      "\tend of episode 71000\n",
      "\tend of episode 72000\n",
      "\tend of episode 73000\n",
      "\tend of episode 74000\n",
      "\tend of episode 75000\n",
      "\tend of episode 76000\n",
      "\tend of episode 77000\n",
      "\tend of episode 78000\n",
      "\tend of episode 79000\n",
      "\tend of episode 80000\n",
      "\tend of episode 81000\n",
      "\tend of episode 82000\n",
      "\tend of episode 83000\n",
      "\tend of episode 84000\n",
      "\tend of episode 85000\n",
      "\tend of episode 86000\n",
      "\tend of episode 87000\n",
      "\tend of episode 88000\n",
      "\tend of episode 89000\n",
      "\tend of episode 90000\n",
      "\tend of episode 91000\n",
      "\tend of episode 92000\n",
      "\tend of episode 93000\n",
      "\tend of episode 94000\n",
      "\tend of episode 95000\n",
      "\tend of episode 96000\n",
      "\tend of episode 97000\n",
      "\tend of episode 98000\n",
      "\tend of episode 99000\n",
      "\n",
      "END OF ALL EPISODES\n",
      "\n",
      "Final Q-values:\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -1.98925781  -1.97851562  -1.98925781  -1.97851562  -1.95703125\n",
      "  -10.97851562]\n",
      " [ -1.828125    -1.65625     -1.828125    -1.65625     -1.3125\n",
      "  -10.65625   ]\n",
      " ...\n",
      " [ -1.3125      -0.625       -1.3125      -1.65625    -10.3125\n",
      "  -10.3125    ]\n",
      " [ -1.95703125  -1.9140625   -1.95703125  -1.9140625  -10.95703125\n",
      "  -10.95703125]\n",
      " [  3.5          0.75         3.5          9.          -5.5\n",
      "   -5.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "lr = 1\n",
    "loops = 100000\n",
    "\n",
    "# run episodes\n",
    "print(\"Starting learning:\")\n",
    "q_learning(loops,lr,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ea682-5ec0-40cc-a35f-9104bb1cd99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 13\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "# test the Q-table values\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "frames = []\n",
    "\n",
    "obs = env.reset()\n",
    "env.s = obs\n",
    "d = False\n",
    "while d == False:\n",
    "    act = np.argmax(Q[obs])\n",
    "    ns,r,d,_=env.step(act)\n",
    "    exp = [obs,ns,r,d]\n",
    "    if reward == -10:\n",
    "        penalities += 1\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': ns,\n",
    "        'action': act,\n",
    "        'reward': r\n",
    "        }\n",
    "    )\n",
    "    epochs += 1\n",
    "    obs = ns\n",
    "   \n",
    "print(f\"Timesteps taken: {epochs}\")\n",
    "print(f\"Penalties incurred: {penalties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a5b48-2e14-4bb3-8970-9efa9b442c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 13\n",
      "State: 85\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08e71f-f7c2-44bb-a32d-5381f53c6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1308\n",
      "State: 410\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "# run n episodes\n",
    "trials = 100\n",
    "frames100 = []\n",
    "\n",
    "for i in range(trials):\n",
    "    \n",
    "    epochs = 0\n",
    "    penalties, reward = 0, 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    env.s = obs\n",
    "    d = False\n",
    "    while d == False:\n",
    "        act = np.argmax(Q[obs])\n",
    "        ns,r,d,_=env.step(act)\n",
    "        exp = [obs,ns,r,d]\n",
    "        if reward == -10:\n",
    "            penalities += 1\n",
    "        frames100.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': ns,\n",
    "            'action': act,\n",
    "            'reward': r\n",
    "            }\n",
    "        )\n",
    "        epochs += 1\n",
    "        obs = ns\n",
    "\n",
    "print_frames(frames100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7b0ab-6b8e-4c27-8354-9c493720129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total penalties for 100 episodes: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total penalties for {trials} episodes: {penalties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519910d7-036b-4071-83d8-95d4ec36d189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
